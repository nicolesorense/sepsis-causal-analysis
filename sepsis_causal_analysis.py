# -*- coding: utf-8 -*-
"""sepsis_causal_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0ACEPGfGV-u4zkFcUpVJ5fkaDTmaJyU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import io
import os
import time
from causallearn.search.ConstraintBased.PC import pc, BackgroundKnowledge
from causallearn.search.ConstraintBased.FCI import fci
from causallearn.utils.GraphUtils import GraphUtils
from causallearn.graph.GeneralGraph import GeneralGraph
from causallearn.graph.GraphNode import GraphNode
from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import statsmodels.api as sm
from scipy.stats import norm
from tqdm import tqdm
from google.cloud import bigquery
import seaborn as sns

"""## Data Loading
Loads MIMIC-IV data from local CSV files and BigQuery queries. Ensure the `data/` directory contains the required files and `GOOGLE_APPLICATION_CREDENTIALS` is set for BigQuery access.
"""

# Set up Google Cloud credentials
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your/service-account-key.json'

client = bigquery.Client()

# Query for d_items
d_items_query = """
    SELECT *
    FROM `physionet-data.mimiciv_3_1_icu.d_items`
"""
d_items_query_job = client.query(d_items_query)
d_items_df = d_items_query_job.to_dataframe()

# Query for d_labitems
d_lab_items_query = """
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.d_labitems`
"""
d_lab_items_query_job = client.query(d_lab_items_query)
d_lab_items_df = d_lab_items_query_job.to_dataframe()

# Query for hospital admissions
query = """
    SELECT DISTINCT hadm_id
    FROM `physionet-data.mimiciv_3_1_icu.icustays`
    ORDER BY FARM_FINGERPRINT(CAST(hadm_id AS STRING))
    LIMIT 15000
"""
query_job = client.query(query)
hadm_ids_df = query_job.to_dataframe()

hadm_ids = hadm_ids_df['hadm_id'].tolist()

hadm_ids_str = ", ".join([str(hadm_id) for hadm_id in hadm_ids])

# Extract admissions info
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.admissions`
    WHERE hadm_id IN ({hadm_ids_str})
"""
query_job = client.query(query)
admissions_df = query_job.to_dataframe()

# Find corresponding patient ids
patient_ids = admissions_df['subject_id'].unique()
patient_ids_str = ", ".join([str(patient_id) for patient_id in patient_ids])

# Extract patients info
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.patients`
    WHERE subject_id IN ({patient_ids_str})
"""
query_job = client.query(query)
patients_df = query_job.to_dataframe()

# Extract procedure events
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_icu.procedureevents`
    WHERE hadm_id IN ({hadm_ids_str})
"""
query_job = client.query(query)
procedureevents_df = query_job.to_dataframe()

# Extract input events
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_icu.inputevents`
    WHERE hadm_id IN ({hadm_ids_str})
"""
query_job = client.query(query)
inputevents_df = query_job.to_dataframe()

# Extract ICU stays
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_icu.icustays`
    WHERE hadm_id IN ({hadm_ids_str})
"""
query_job = client.query(query)
icustays_df = query_job.to_dataframe()

# Extract ICU transfers
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.transfers`
    WHERE hadm_id IN ({hadm_ids_str}) AND careunit IN ('Medical Intensive Care Unit (MICU)','Medical/Surgical Intensive Care Unit (MICU/SICU)', 'Surgical Intensive Care Unit (SICU)', 'Cardiac Vascular Intensive Care Unit (CVICU)', 'Trauma SICU (TSICU)', 'Neuro Surgical Intensive Care Unit (Neuro SICU)')
"""
query_job = client.query(query)
transfers_df = query_job.to_dataframe()

# Sort transfers df by subject, admission, and time
transfers_df = transfers_df.sort_values(by=['subject_id', 'hadm_id', 'intime'])

# Filter the df for transfers
transfers_filtered = transfers_df[transfers_df['eventtype'] == 'transfer']

# Add a column for previous care unit
transfers_filtered['previous_careunit'] = transfers_filtered.groupby(['subject_id', 'hadm_id'])['careunit'].shift(1)

# Identify inter-unit transfers
interunit_transfers = transfers_filtered[transfers_filtered['careunit'] != transfers_filtered['previous_careunit']]
for i, row in interunit_transfers.iterrows():
  if pd.notna(row['previous_careunit']):
    interunit_transfers.at[i, 'transfer_occurred'] = True
  else:
    interunit_transfers.at[i, 'transfer_occurred'] = False

# Add tranfer_occurred column to transfers_df
transfers_df = pd.merge(transfers_df, interunit_transfers, how='left')

# Change all NaN transfers to False
for i, row in transfers_df.iterrows():
  if row['transfer_occurred'] != True:
    transfers_df.at[i, 'transfer_occurred'] = False

# Aggregate transfers with improved handling of intime
transfers_df = transfers_df.sort_values(by=['hadm_id', 'intime'])

# Separate into true transfers and other
true_transfers = transfers_df[transfers_df['transfer_occurred'] == True]
other_transfers = transfers_df[transfers_df['transfer_occurred'] == False]

# Keep only the first true transfer for each hadm_id
true_transfers = true_transfers.drop_duplicates(subset=['hadm_id'], keep='first')

# For non-transfer hadm_ids, use the first ICU stay intime from icustays_df
non_transfer_hadm_ids = set(transfers_df['hadm_id']) - set(true_transfers['hadm_id'])
filtered_other_transfers = other_transfers[other_transfers['hadm_id'].isin(non_transfer_hadm_ids)]
filtered_other_transfers = filtered_other_transfers.drop_duplicates(subset=['hadm_id'], keep='first')

# Merge with icustays_df to get the first ICU intime for non-transfer admissions
icustays_df = icustays_df.sort_values(by=['hadm_id', 'intime'])
first_icu_stay = icustays_df.groupby('hadm_id').agg({
    'intime': 'min',
    'stay_id': 'first'
}).reset_index().rename(columns={'intime': 'icu_intime'})

# Replace intime in filtered_other_transfers with icu_intime
filtered_other_transfers = filtered_other_transfers.drop(columns=['intime']).merge(
    first_icu_stay[['hadm_id', 'icu_intime']], on='hadm_id', how='left'
)
filtered_other_transfers = filtered_other_transfers.rename(columns={'icu_intime': 'intime'})

# Combine the two groups
aggregated_transfers = pd.concat([true_transfers, filtered_other_transfers]).sort_values(by=['hadm_id', 'intime']).reset_index(drop=True)

# Debug: Check for missing intimes
print("Missing intimes:", aggregated_transfers['intime'].isna().sum())
aggregated_transfers = aggregated_transfers.dropna(subset=['intime'])

# Extract matching data from labevents
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.labevents`
    WHERE hadm_id IN ({hadm_ids_str})
"""

query_job = client.query(query)
labevents_df = query_job.to_dataframe()

# Extract diagnoses for sepsis ICD codes
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_hosp.diagnoses_icd`
    WHERE hadm_id IN ({hadm_ids_str})
"""

query_job = client.query(query)
diagnoses_df = query_job.to_dataframe()

# Extract matching data from chartevents
query = f"""
    SELECT *
    FROM `physionet-data.mimiciv_3_1_icu.chartevents`
    WHERE hadm_id IN ({hadm_ids_str})
"""

query_job = client.query(query)
chartevents_df = query_job.to_dataframe()

# Extract data needed for sofa scores (sofa method)
query = f"""
  SELECT
    i.stay_id,
    i.hadm_id,
    s.sepsis3,
    s.suspected_infection_time
  FROM `physionet-data.mimiciv_3_1_icu.icustays` i
  LEFT JOIN `physionet-data.mimiciv_derived.sepsis3` s
    ON i.stay_id = s.stay_id
  WHERE i.hadm_id IN ({hadm_ids_str})
  ORDER BY i.stay_id, s.suspected_infection_time;
"""

query_job = client.query(query)

sepsis_df = query_job.to_dataframe()

# Filter for post-transfer sepsis
sepsis_df['suspected_infection_time'] = pd.to_datetime(sepsis_df['suspected_infection_time'])
sepsis_merged = pd.merge(sepsis_df, aggregated_transfers[['hadm_id', 'transfer_occurred', 'intime']], on='hadm_id', how='left')
print(sepsis_merged.head(2))
sepsis_merged['sepsis_occurred'] = (sepsis_merged['sepsis3'] == True) & (sepsis_merged[
    'suspected_infection_time'] > sepsis_merged['intime']) & (sepsis_merged['transfer_occurred'] == True)

# Create dataset for causal inference
# Initialize with hadm_id
wrangled_df = pd.DataFrame({'hadm_id': hadm_ids_df['hadm_id']})

# Add treatment variable: inter-unit ICU transfer
wrangled_df = pd.merge(wrangled_df, aggregated_transfers, on='hadm_id', how='left')

# Fill missing transfer information for non-transfers
wrangled_df['transfer_occurred'] = wrangled_df['transfer_occurred'].fillna(False)

# Add response variable: sepsis outcome (using SOFA definition)
sepsis_merged = sepsis_merged.drop(columns = ['intime', 'transfer_occurred'])
wrangled_df = pd.merge(wrangled_df, sepsis_merged, on='hadm_id', how='left')
wrangled_df['sepsis_occurred'] = ~wrangled_df['suspected_infection_time'].isna()

# Drop sepsis3 column
wrangled_df.drop(columns=['sepsis3'], inplace=True)

# Rename suspected infection time
wrangled_df.rename(columns={'suspected_infection_time': 'sepsis_onset_time'}, inplace=True)

# Final wrangled dataset
wrangled_df.reset_index(drop=True, inplace=True)

# Merge demographics and admission details
confounders = admissions_df[['subject_id', 'hadm_id', 'admittime']].copy()
confounders = confounders.merge(patients_df[['subject_id', 'gender', 'anchor_age']], on='subject_id', how='left')

# Drop columns not needed
confounders = confounders.drop(['admittime'], axis=1)

# Define comorbidity ICD code groups
comorbidities = {
    'diabetes': ['E10', 'E11'],
    'hypertension': ['I10', 'I11', 'I12'],
    'heart_disease': ['I20', 'I21', 'I22', 'I50']
}

# Add binary flags for each comorbidity
for comorbidity, codes in comorbidities.items():
    diagnoses_df[comorbidity] = diagnoses_df['icd_code'].str.startswith(tuple(codes))

# Aggregate comorbidities at the admission level
comorbidity_flags = diagnoses_df.groupby('hadm_id').agg({
    'diabetes': 'max',
    'hypertension': 'max',
    'heart_disease': 'max'
}).reset_index()

# Ensure datetime columns are properly formatted
labevents_df['charttime'] = pd.to_datetime(labevents_df['charttime'])
wrangled_df['intime'] = pd.to_datetime(wrangled_df['intime'])
wrangled_df['sepsis_onset_time'] = pd.to_datetime(wrangled_df['sepsis_onset_time'])

# Merge transfer and sepsis onset times with lab events
labevents_df = labevents_df.merge(
    wrangled_df[['hadm_id', 'intime', 'sepsis_onset_time']], on='hadm_id', how='left'
)

# Filter for creatinine measurements
creatinine_itemid = 50912  # Item ID for creatinine

# Calculate average creatinine before transfer
creatinine_before_transfer = labevents_df[
    (labevents_df['itemid'] == creatinine_itemid) &
    (labevents_df['charttime'] < labevents_df['intime'])
]
creatinine_before_transfer = (
    creatinine_before_transfer.groupby('hadm_id')['valuenum']
    .mean()
    .reset_index()
    .rename(columns={'valuenum': 'avg_creatinine_before_transfer'})
)

# Calculate average creatinine between transfer and sepsis
creatinine_between_transfer_and_sepsis = labevents_df[
    (labevents_df['itemid'] == creatinine_itemid) &
    (labevents_df['charttime'] >= labevents_df['intime']) &
    (labevents_df['charttime'] < labevents_df['sepsis_onset_time'])
]
creatinine_between_transfer_and_sepsis = (
    creatinine_between_transfer_and_sepsis.groupby('hadm_id')['valuenum']
    .mean()
    .reset_index()
    .rename(columns={'valuenum': 'avg_creatinine_between_transfer_and_sepsis'})
)

# Merge results into wrangled_df
wrangled_df = wrangled_df.merge(creatinine_before_transfer, on='hadm_id', how='left')
wrangled_df = wrangled_df.merge(creatinine_between_transfer_and_sepsis, on='hadm_id', how='left')

# Final wrangled_df contains average creatinine values for the specified time windows

# Define GCS item IDs
gcs_items = {
    'eye': [220739],      # Eye response
    'motor': [223901],    # Motor response
    'verbal': [223900]    # Verbal response
}

# Filter the chartevents data for GCS item IDs
gcs_data = chartevents_df[chartevents_df['itemid'].isin(gcs_items['eye'] + gcs_items['motor'] + gcs_items['verbal'])]

# Merge GCS data with wrangled_df to get only the records for included admissions
gcs_data = gcs_data[gcs_data['hadm_id'].isin(wrangled_df['hadm_id'])]

# Convert 'charttime' to datetime if not already
gcs_data['charttime'] = pd.to_datetime(gcs_data['charttime'])

# Merge with transfer and sepsis times to filter GCS before and after transfer
gcs_data = pd.merge(
    gcs_data,
    wrangled_df[['hadm_id', 'transfer_id', 'intime', 'sepsis_onset_time']],
    on='hadm_id',
    how='left'
)

# Pivot the data to have one column for each GCS component (eye, motor, verbal)
gcs_data_pivot = gcs_data.pivot_table(
    index=['hadm_id', 'charttime', 'intime', 'sepsis_onset_time'],
    columns='itemid',
    values='valuenum'
).reset_index()

# Rename columns for clarity
gcs_data_pivot.rename(
    columns={
        220739: 'gcs_eye',
        223901: 'gcs_motor',
        223900: 'gcs_verbal'
    },
    inplace=True
)

# Ensure no missing columns in the pivoted data
required_columns = ['gcs_eye', 'gcs_motor', 'gcs_verbal']
for col in required_columns:
    if col not in gcs_data_pivot:
        gcs_data_pivot[col] = 0  # Assign 0 if a column is missing

# Calculate total GCS score
gcs_data_pivot['gcs_total'] = gcs_data_pivot[required_columns].sum(axis=1)

# Separate GCS data into before transfer and after transfer but before sepsis
gcs_before_transfer = gcs_data_pivot[
    gcs_data_pivot['charttime'] < gcs_data_pivot['intime']
]
gcs_after_transfer_before_sepsis = gcs_data_pivot[
    (gcs_data_pivot['charttime'] >= gcs_data_pivot['intime']) &
    (gcs_data_pivot['charttime'] < gcs_data_pivot['sepsis_onset_time'])
]

# Debugging: Check if the filtered DataFrames are empty
if gcs_before_transfer.empty:
    print("Warning: gcs_before_transfer is empty.")
if gcs_after_transfer_before_sepsis.empty:
    print("Warning: gcs_after_transfer_before_sepsis is empty.")

# Get the average GCS score before transfer for each patient
gcs_before_transfer_avg = gcs_before_transfer.groupby('hadm_id')['gcs_total'].mean().reset_index()
gcs_before_transfer_avg.rename(columns={'gcs_total': 'gcs_before_transfer'}, inplace=True)

# Get the average GCS score after transfer but before sepsis for each patient
gcs_after_transfer_before_sepsis_avg = gcs_after_transfer_before_sepsis.groupby('hadm_id')['gcs_total'].mean().reset_index()
gcs_after_transfer_before_sepsis_avg.rename(columns={'gcs_total': 'gcs_after_transfer_before_sepsis'}, inplace=True)

# Debugging: Check column names before merging
print("gcs_before_transfer_avg columns:", gcs_before_transfer_avg.columns)
print("gcs_after_transfer_before_sepsis_avg columns:", gcs_after_transfer_before_sepsis_avg.columns)

# Merge the GCS scores back into wrangled_df
wrangled_df = pd.merge(wrangled_df, gcs_before_transfer_avg, on='hadm_id', how='left')
wrangled_df = pd.merge(wrangled_df, gcs_after_transfer_before_sepsis_avg, on='hadm_id', how='left')

# Fill missing values with 0 or other appropriate value
wrangled_df['gcs_before_transfer'] = wrangled_df['gcs_before_transfer'].fillna(0)
wrangled_df['gcs_after_transfer_before_sepsis'] = wrangled_df['gcs_after_transfer_before_sepsis'].fillna(0)

# Combine all confounders
confounders = confounders.merge(comorbidity_flags, on='hadm_id', how='left')

# Merge with wrangled_df
wrangled_df = wrangled_df.merge(confounders, on='hadm_id', how='left')

# Ensure datetime columns are in proper datetime format
icustays_df['intime'] = pd.to_datetime(icustays_df['intime'])
icustays_df['outtime'] = pd.to_datetime(icustays_df['outtime'])

# For each ICU stay, calculate the length of stay
icustays_df['icu_los_individual'] = (icustays_df['outtime'] - icustays_df['intime']).dt.total_seconds() / 3600  # Convert to hours

# Sort ICU stays by admission ID and intime
icustays_df = icustays_df.sort_values(by=['hadm_id', 'intime'])

# Calculate cumulative ICU length of stay for each admission up to (but not including) the specific transfer
def calculate_cumulative_los_before_transfer(row, icustays_df):
    hadm_id = row['hadm_id']
    transfer_time = row['intime']

    # Filter ICU stays for the same hospital admission and before the specific transfer time
    relevant_stays = icustays_df[(icustays_df['hadm_id'] == hadm_id) & (icustays_df['intime'] < transfer_time)]

    # Calculate cumulative length of stay
    cumulative_los = relevant_stays['icu_los_individual'].sum()
    return cumulative_los

# Apply the calculation to the wrangled_df
wrangled_df['icu_los_before_transfer'] = wrangled_df.apply(
    lambda row: calculate_cumulative_los_before_transfer(row, icustays_df),
    axis=1
)

# Fill missing ICU LOS with 0 for admissions without prior ICU stays
wrangled_df['icu_los_before_transfer'] = wrangled_df['icu_los_before_transfer'].fillna(0)

# Count comorbidities for each admission
comorbidity_count = diagnoses_df.groupby('hadm_id')['icd_code'].nunique().reset_index()
comorbidity_count.rename(columns={'icd_code': 'comorbidity_count'}, inplace=True)

# Merge with wrangled_df
wrangled_df = wrangled_df.merge(comorbidity_count, on='hadm_id', how='left')

# Search for antibiotics in D_ITEMS
antibiotic_items = d_items_df[d_items_df['label'].str.contains('antibiotic|antimicrobial|vancomycin|ceftriaxone', case=False, na=False)]
antibiotic_item_ids = antibiotic_items['itemid'].tolist()

# Identify first antibiotic administration time for each admission
antibiotics = inputevents_df[inputevents_df['itemid'].isin(antibiotic_item_ids)]  # Replace with specific item IDs
antibiotics_timing = antibiotics.groupby('hadm_id')['starttime'].min().reset_index()
antibiotics_timing.rename(columns={'starttime': 'first_antibiotic_time'}, inplace=True)

# Merge with wrangled_df
wrangled_df = wrangled_df.merge(antibiotics_timing, on='hadm_id', how='left')

# Add binary flags for whether antibiotics were administered
wrangled_df['antibiotic_used'] = ~wrangled_df['first_antibiotic_time'].isna()

# Add dual flags for antibiotic administration
# 1. Antibiotics administered before transfer
wrangled_df['antibiotic_before_transfer'] = (
    wrangled_df['first_antibiotic_time'] < wrangled_df['intime']
)
wrangled_df['antibiotic_before_transfer'] = wrangled_df['antibiotic_before_transfer'].fillna(False)

# 2. Antibiotics administered between transfer and sepsis
wrangled_df['antibiotic_after_transfer_before_sepsis'] = (
    (wrangled_df['first_antibiotic_time'] >= wrangled_df['intime']) &
    (wrangled_df['first_antibiotic_time'] < wrangled_df['sepsis_onset_time'])
)
wrangled_df['antibiotic_after_transfer_before_sepsis'] = wrangled_df['antibiotic_after_transfer_before_sepsis'].fillna(False)

# Search for vasopressors in D_ITEMS
vasopressor_items = d_items_df[d_items_df['label'].str.contains('norepinephrine|epinephrine|vasopressin|dopamine|dobutamine', case=False, na=False)]
vasopressor_item_ids = vasopressor_items['itemid'].tolist()

# Find the first time vasopressors were administered
vasopressor_timing = inputevents_df[inputevents_df['itemid'].isin(vasopressor_item_ids)]
vasopressor_timing = vasopressor_timing.groupby('hadm_id')['starttime'].min().reset_index()
vasopressor_timing.rename(columns={'starttime': 'first_vasopressor_time'}, inplace=True)

# Merge with wrangled_df
wrangled_df = wrangled_df.merge(vasopressor_timing, on='hadm_id', how='left')

# Add binary flags for whether vasopressors were used
wrangled_df['vasopressor_used'] = ~wrangled_df['first_vasopressor_time'].isna()

# Add dual flags for vasopressor administration
# 1. Vasopressors administered before transfer
wrangled_df['vasopressor_before_transfer'] = (
    wrangled_df['first_vasopressor_time'] < wrangled_df['intime']
)
wrangled_df['vasopressor_before_transfer'] = wrangled_df['vasopressor_before_transfer'].fillna(False)

# 2. Vasopressors administered between transfer and sepsis
wrangled_df['vasopressor_after_transfer_before_sepsis'] = (
    (wrangled_df['first_vasopressor_time'] >= wrangled_df['intime']) &
    (wrangled_df['first_vasopressor_time'] < wrangled_df['sepsis_onset_time'])
)
wrangled_df['vasopressor_after_transfer_before_sepsis'] = wrangled_df['vasopressor_after_transfer_before_sepsis'].fillna(False)

# Final binary flags
print(wrangled_df[['vasopressor_used', 'vasopressor_before_transfer', 'vasopressor_after_transfer_before_sepsis']].head())

# Search for invasive procedures in D_ITEMS
invasive_procedure_items = d_items_df[d_items_df['label'].str.contains('intubation|ventilation|central line|arterial catheter', case=False, na=False)]
invasive_procedure_item_ids = invasive_procedure_items['itemid'].tolist()

# Find the first time invasive procedures were performed
procedure_timing = procedureevents_df[procedureevents_df['itemid'].isin(invasive_procedure_item_ids)]
procedure_timing = procedure_timing.groupby('hadm_id')['starttime'].min().reset_index()
procedure_timing.rename(columns={'starttime': 'first_procedure_time'}, inplace=True)

# Merge with wrangled_df
wrangled_df = wrangled_df.merge(procedure_timing, on='hadm_id', how='left')

# Add binary flags for whether procedure was done
wrangled_df['procedure_performed'] = ~wrangled_df['first_procedure_time'].isna()

# 1. Procedure before transfer
wrangled_df['procedure_before_transfer'] = (
    wrangled_df['first_procedure_time'] < wrangled_df['intime']
)
wrangled_df['procedure_before_transfer'] = wrangled_df['procedure_before_transfer'].fillna(False)

# 2. Procedure between transfer and sepsis
wrangled_df['procedure_after_transfer_before_sepsis'] = (
    (wrangled_df['first_procedure_time'] >= wrangled_df['intime']) &
    (wrangled_df['first_procedure_time'] < wrangled_df['sepsis_onset_time'])
)
wrangled_df['procedure_after_transfer_before_sepsis'] = wrangled_df['procedure_after_transfer_before_sepsis'].fillna(False)

# Normalize continuous columns
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
continuous_columns = ['comorbidity_count', 'icu_los_before_transfer', 'anchor_age', 'avg_creatinine_before_transfer', 'avg_creatinine_between_transfer_and_sepsis', 'gcs_before_transfer', 'gcs_after_transfer_before_sepsis']
wrangled_df[continuous_columns] = scaler.fit_transform(wrangled_df[continuous_columns])

# Fill in missing values for relevant columns
categorical_columns = ['gender']
for col in categorical_columns:
    wrangled_df[col] = wrangled_df[col].fillna('Unknown')

# Ensure datetime columns are in the correct format
wrangled_df['outtime'] = pd.to_datetime(wrangled_df['outtime'])
wrangled_df['sepsis_onset_time'] = pd.to_datetime(wrangled_df['sepsis_onset_time'])

# Filter rows where transfers occurred before sepsis onset
filtered_dataset = wrangled_df[
    (wrangled_df['outtime'] <= wrangled_df['sepsis_onset_time']) | wrangled_df['sepsis_onset_time'].isna()
]

# Reset the index for clarity
filtered_dataset.reset_index(drop=True, inplace=True)

final_df = filtered_dataset.drop(['hadm_id', 'subject_id_x', 'transfer_id', 'eventtype', 'careunit', 'intime', 'outtime', 'previous_careunit', 'sepsis_onset_time', 'subject_id_y', 'first_antibiotic_time', 'first_vasopressor_time', 'vasopressor_used', 'first_procedure_time', 'procedure_performed'], axis=1)

# Use one-hot encoding for categorical variables
final_df = pd.get_dummies(final_df, columns=['transfer_occurred', 'sepsis_occurred', 'gender'])

# Identify redundant dummy variables ending in "_False"
redundant_columns = [col for col in final_df.columns if col.endswith('_False') or col.endswith('_M')]

# Drop redundant dummy variables
dataset = final_df.drop(columns=redundant_columns)

# Impute missing values
continuous_vars = ['avg_creatinine_before_transfer', 'avg_creatinine_between_transfer_and_sepsis', 'comorbidity_count', 'hypertension', 'heart_disease', 'icu_los_before_transfer', 'diabetes']
imputer = KNNImputer(n_neighbors=5)
dataset[continuous_vars] = imputer.fit_transform(dataset[continuous_vars])

"""Causal discovery using the PC algorithm"""

corr_matrix = dataset.corr().abs()

# Select the upper triangle of the correlation matrix
upper_triangle = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

# Find pairs of features with correlation = 1.0
perfectly_correlated = [
    (column, index) for column in upper_triangle.columns
    for index in upper_triangle.index
    if upper_triangle.loc[index, column] == 1.0
]

# Display the perfectly correlated pairs
print("Perfectly correlated variable pairs:")
for pair in perfectly_correlated:
    print(pair)

data = dataset.to_numpy()

# Convert the data to numeric
data = data.astype(float)

# Define cache path for kci p-values
citest_cache_file = "/data/citest_cache_mimiciv_kci.json"

# Check if the cache file exists and delete it if it does
if os.path.exists(citest_cache_file):
    print(f"Deleting existing cache file: {citest_cache_file}")
    os.remove(citest_cache_file)
    print("Cache file deleted.")
else:
    print(f"Cache file not found: {citest_cache_file}. No need to delete.")

# Define variable categories (11 variables)
before_transfer_vars = [
    'gcs_before_transfer',  # Key confounder
    'avg_creatinine_before_transfer',  # Kidney function confounder
    'vasopressor_before_transfer'  # Patient severity confounder
]

time_invariant_vars = [
    'comorbidity_count',  # Key confounder
    'anchor_age'  # Age confounder
]

transfer_var = 'transfer_occurred_True'  # Treatment

after_transfer_vars = [
    'gcs_after_transfer_before_sepsis',  # Mediator
    'avg_creatinine_between_transfer_and_sepsis',  # Mediator
    'antibiotic_after_transfer_before_sepsis',  # Mediator
    'vasopressor_after_transfer_before_sepsis',  # Mediator
    'sepsis_occurred_True'  # Outcome
]

# Define after-transfer variables before sepsis (for forbidding sepsis edges)
after_transfer_before_sepsis_vars = [
    'gcs_after_transfer_before_sepsis',
    'avg_creatinine_between_transfer_and_sepsis',
    'antibiotic_after_transfer_before_sepsis',
    'vasopressor_after_transfer_before_sepsis'
]

# Combine variables for causal discovery (11 variables, excludes propensity_score, weights, weights_norm)
core_vars = before_transfer_vars + time_invariant_vars + [transfer_var] + after_transfer_vars

# Subsample the dataset
sample_size = 2000  # Adjust if needed
data_for_causal_discovery = dataset[core_vars].sample(n=min(sample_size, len(dataset)), random_state=42).copy()

# Use kci test (no discretization needed)
independence_test = 'kci'

# Create a temporary graph to get Node objects
nodes = [GraphNode(var) for var in data_for_causal_discovery.columns]
graph = GeneralGraph(nodes)

# Map column names to Node objects
node_map = {var: node for var, node in zip(data_for_causal_discovery.columns, nodes)}

# Combine before-transfer, time-invariant, transfer, and after-transfer-before-sepsis variables as forbidden targets for sepsis
forbidden_targets_for_sepsis = before_transfer_vars + time_invariant_vars + [transfer_var] + after_transfer_before_sepsis_vars
forbidden_targets = before_transfer_vars + time_invariant_vars + [transfer_var]

# Create background knowledge object
bk = BackgroundKnowledge()

# 1. Forbid edges from transfer_occurred_True to before-transfer and time-invariant variables
transfer_node = node_map[transfer_var]
for target_var in before_transfer_vars + time_invariant_vars:
    target_node = node_map[target_var]
    bk.add_forbidden_by_node(transfer_node, target_node)

# 2. Forbid edges from after-transfer variables to before-transfer variables, time-invariant variables, and transfer variable
for source_var in after_transfer_vars:
    source_node = node_map[source_var]
    for target_var in forbidden_targets:
        target_node = node_map[target_var]
        bk.add_forbidden_by_node(source_node, target_node)

# 3. Forbid edges from sepsis_occurred_True to before-sepsis variables
sepsis_node = node_map['sepsis_occurred_True']
for target_var in forbidden_targets_for_sepsis:
    target_node = node_map[target_var]
    bk.add_forbidden_by_node(sepsis_node, target_node)

# Convert data to numpy array
data = data_for_causal_discovery.to_numpy().astype(float)

# Run PC algorithm with kci, caching, and limited depth
start_time = time.time()
pc_result = pc(
    data,
    independence_test_method=independence_test,
    background_knowledge=bk,
    alpha=0.2,
    max_conditioning_variables=1,  # Limit conditioning set to 1 variable
    cache_path=citest_cache_file,  # Cache p-values
    verbose=True  # Monitor conditioning set sizes
)
print(f"PC runtime: {time.time() - start_time} seconds")

# Visualize PC graph
node_labels = {i: column_name for i, column_name in enumerate(data_for_causal_discovery.columns)}
pc_result.draw_pydot_graph(labels=node_labels)

# Run FCI algorithm with timeout
start_time = time.time()

fci_g, fci_edges = fci(
    data,
    independence_test_method=independence_test,
    background_knowledge=bk,
    alpha=0.2,
    max_conditioning_variables=1,  # Limit conditioning set to 1 variable
    cache_path=citest_cache_file,  # Cache p-values
    verbose=True  # Monitor conditioning set sizes
)


# Visualize FCI graph if completed
if fci_g is not None:
    pyd = GraphUtils.to_pydot(fci_g, labels=node_labels)
    tmp_png = pyd.create_png(f="png")
    fp = io.BytesIO(tmp_png)
    img = mpimg.imread(fp, format='png')
    plt.axis('off')
    plt.imshow(img)
    plt.show()

# Print edges for verification
print("PC Graph Edges:")
for i in range(pc_result.G.get_num_nodes()):
    for j in range(pc_result.G.get_num_nodes()):
        if pc_result.G.graph[i, j] != 0:
            print(f"{data_for_causal_discovery.columns[i]} -> {data_for_causal_discovery.columns[j]}")

if fci_edges:
    print("\nFCI Graph Edges:")
    for edge in fci_edges:
        print(edge)

# Check data distribution in subsample
print("\nSubsample Distribution:")
print(data_for_causal_discovery['transfer_occurred_True'].value_counts(normalize=True))
print(data_for_causal_discovery['sepsis_occurred_True'].value_counts(normalize=True))

# Define variables to ensure correct mapping
before_transfer_vars = [
    'gcs_before_transfer',
    'avg_creatinine_before_transfer',
    'vasopressor_before_transfer',
    'antibiotic_before_transfer'
]
time_invariant_vars = [
    'comorbidity_count',
    'anchor_age',
    'heart_disease',
    'gender_F'
]
transfer_var = 'transfer_occurred_True'
after_transfer_before_sepsis_vars = [
    'gcs_after_transfer_before_sepsis',
    'avg_creatinine_between_transfer_and_sepsis',
    'antibiotic_after_transfer_before_sepsis',
    'vasopressor_after_transfer_before_sepsis',
    'procedure_after_transfer_before_sepsis'
]
sepsis_var = 'sepsis_occurred_True'
forbidden_targets_for_sepsis = (
    before_transfer_vars + time_invariant_vars + [transfer_var] + after_transfer_before_sepsis_vars
)

# Map variable names and indices to nodes
node_map = {var: node for var, node in zip(core_vars, nodes)}
index_to_var = {i: var for i, var in enumerate(core_vars)}
var_to_index = {var: i for i, var in enumerate(core_vars)}

# Post-process FCI graph to remove forbidden edges
if fci_g is not None:
    for edge in fci_edges[:]:  # Copy to avoid modifying while iterating
        source_name = edge.get_node1().get_name()
        target_name = edge.get_node2().get_name()
        print(f'Source: {source_name}')
        print(f'Target: {target_name}')
        # Initialize with node names
        source_var = source_name
        target_var = target_name
        # Map X1, X2, ... to variable names if needed
        if source_name.startswith('X'):
            try:
                index = int(source_name.replace('X', '')) - 1  # Adjust for X1 -> index 0
                source_var = index_to_var.get(index, source_name)
            except ValueError:
                print(f"Warning: Cannot parse source_name {source_name}")
        if target_name.startswith('X'):
            try:
                index = int(target_name.replace('X', '')) - 1  # Adjust for X1 -> index 0
                target_var = index_to_var.get(index, target_name)
            except ValueError:
                print(f"Warning: Cannot parse target_name {target_name}")
        print(f'Mapped Source: {source_var}')
        print(f'Mapped Target: {target_var}')
        # Remove forbidden edges
        if (source_var == transfer_var and target_var in before_transfer_vars) or \
           (source_var == sepsis_var and target_var in forbidden_targets_for_sepsis):
            print(f"Removing forbidden FCI edge: {source_var} -> {target_var}")
            fci_g.remove_edge(edge)
            fci_edges.remove(edge)

# Visualize FCI graph if completed
if fci_g is not None:
    pyd = GraphUtils.to_pydot(fci_g, labels=node_labels)
    tmp_png = pyd.create_png(f="png")
    fp = io.BytesIO(tmp_png)
    img = mpimg.imread(fp, format='png')
    plt.axis('off')
    plt.imshow(img)
    plt.show()

treatment = dataset['transfer_occurred_True']
outcome = dataset['sepsis_occurred_True']
covariates = dataset[['gcs_before_transfer', 'comorbidity_count', 'avg_creatinine_before_transfer']]

def compute_smd(covariates_treated, covariates_control):
    treated_mean = covariates_treated.mean()
    control_mean = covariates_control.mean()
    pooled_std = ((covariates_treated.std()**2 + covariates_control.std()**2) / 2)**0.5
    return abs(treated_mean - control_mean) / pooled_std

covariate_balance = {col: compute_smd(
    dataset[dataset['transfer_occurred_True'] == 1][col],
    dataset[dataset['transfer_occurred_True'] == 0][col])
    for col in covariates.columns}
print(covariate_balance)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
covariates_scaled = scaler.fit_transform(covariates)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(covariates_scaled, treatment)

propensity_scores = model.predict_proba(covariates_scaled)[:, 1]
dataset['propensity_score'] = propensity_scores

from sklearn.neighbors import NearestNeighbors

treated = dataset[treatment == 1]
control = dataset[treatment == 0]

nn = NearestNeighbors(n_neighbors=1)
nn.fit(control[['propensity_score']])

distances, indices = nn.kneighbors(treated[['propensity_score']])
matched_controls = control.iloc[indices.flatten()]
matched_treated = treated.reset_index(drop=True)

# Assign subclass (pair ID)
matched_treated['subclass'] = np.arange(len(matched_treated))
matched_controls['subclass'] = np.arange(len(matched_controls))

matched_dataset = pd.concat([matched_treated, matched_controls], axis=0)

def compute_smd(covariates_treated, covariates_control):
    treated_mean = covariates_treated.mean()
    control_mean = covariates_control.mean()
    pooled_std = ((covariates_treated.std()**2 + covariates_control.std()**2) / 2)**0.5
    return abs(treated_mean - control_mean) / pooled_std

covariate_balance_after = {col: compute_smd(
    matched_dataset[matched_dataset['transfer_occurred_True'] == 1][col],
    matched_dataset[matched_dataset['transfer_occurred_True'] == 0][col])
    for col in covariates.columns}
print(covariate_balance_after)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set seaborn style for a modern look
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 12

# Prepare data for love plot
covariates_list = ['GCS Before', 'Comorbidity', 'Creatinine']  # Shortened names
smd_before = [covariate_balance[col] for col in covariates.columns]
smd_after = [covariate_balance_after[col] for col in covariates.columns]

# Create love plot
plt.figure(figsize=(10, 6))

# Plot SMDs with distinct markers and colors
y_pos = np.arange(len(covariates_list))
plt.plot(smd_before, y_pos, 'o', label='Before Matching', color='#e74c3c', markersize=10, alpha=0.8)
plt.plot(smd_after, y_pos, '^', label='After Matching', color='#2ecc71', markersize=10, alpha=0.8)

# Add shaded area for SMD < 0.1
plt.axvspan(0, 0.1, color='#ecf0f1', alpha=0.3, label='Good Balance (SMD < 0.1)')
plt.axvline(x=0.1, color='#7f8c8d', linestyle='--', linewidth=1.5)

# Customize plot
plt.yticks(y_pos, covariates_list, fontsize=12)
plt.xlabel('Standardized Mean Difference (SMD)', fontsize=14)
plt.title('Covariate Balance: Before vs. After Propensity Score Matching', fontsize=16, pad=15)
plt.legend(fontsize=12, loc='upper right')
plt.grid(True, axis='x', linestyle='--', alpha=0.5)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

def getMeanDiff(df):
  mean_diff = df[df['transfer_occurred_True'] == 1]['sepsis_occurred_True'].mean() - \
      df[df['transfer_occurred_True'] == 0]['sepsis_occurred_True'].mean()
  return mean_diff
ate_est = getMeanDiff(matched_dataset)
print("Estimated ATE:", ate_est)

att = matched_treated['sepsis_occurred_True'].mean() - matched_controls['sepsis_occurred_True'].mean()
print("Estimated ATT:", att)

# Get the variance for the confidence interval
def getMeanDiffVarPaired(df):
  J = len(df['subclass'].unique())
  tau_j = []
  for subclass in df['subclass'].unique():
    pair_data = df[df['subclass'] == subclass]
    if len(pair_data) == 2:
      tau = getMeanDiff(pair_data)
      tau_j.append(tau)

  tau_j = np.array(tau_j)
  var_paired = sum((tau_j - np.mean(tau_j))**2)/ (J*(J-1))
  return var_paired

var_paired = getMeanDiffVarPaired(matched_dataset)
print("Estimated variance of ATE:", var_paired)

# Generate a confidence interval for the estimate
ci = (ate_est - norm.ppf(1 - 0.05/2) * np.sqrt(var_paired), ate_est + norm.ppf(1 - 0.05/2) * np.sqrt(var_paired))
print("Confidence interval:", ci)

# Compute the weights for the IPW estimate
weights = treatment / propensity_scores + (1 - treatment) / (1 - propensity_scores)
dataset['weights'] = weights

import statsmodels.api as sm
treatment = treatment.astype(float)
outcome = outcome.astype(float)
X = sm.add_constant(treatment)  # Add intercept
model = sm.WLS(outcome, X, weights=weights).fit()
print("Estimated ATE (IPTW):", model.params[1])

def compute_ipw_ate(df):
  treatment = df['transfer_occurred_True'].astype(float)
  outcome = df['sepsis_occurred_True'].astype(float)
  propensity_score = dataset['propensity_score']

  # Compute IPW weights
  weights = treatment / propensity_score + (1 - treatment) / (1 - propensity_score)

  # Normalize weights
  sum_weights_treated = weights[treatment == 1].sum()
  sum_weights_control = weights[treatment == 0].sum()
  weights_norm = np.where(
      treatment == 1,
      weights / sum_weights_treated,
      weights / sum_weights_control
  )

  df['weights_norm'] = weights_norm

  # Fit weighted regression
  X = sm.add_constant(treatment) # Add intercept
  model = sm.WLS(outcome, X, weights=weights_norm).fit(cov_type='HC2')

  # Extract ATE, se, and variance
  ate = model.params[1] # Treatment coefficient
  se = model.bse[1] # Robust standard error (HC2)
  variance = se**2

  # Compute CI
  z = norm.ppf(1 - 0.05/2)
  ci = (ate - z * se, ate + z * se)
  return ate, variance, ci

ipw_ate, ipw_variance, ipw_ci = compute_ipw_ate(dataset)
print("Estimated ATE (IPTW):", ipw_ate)
print("Estimated variance of ATE (IPTW):", ipw_variance)
print("Confidence interval (IPTW):", ipw_ci)

def compute_dr_ate(df, covariates=['gcs_before_transfer', 'comorbidity_count'], n_folds=5):
    """Compute doubly robust ATE with k-fold cross-fitting and variance."""
    # Ensure data types and handle missing values
    df = df.copy()
    df['transfer_occurred_True'] = df['transfer_occurred_True'].astype(float)
    df['sepsis_occurred_True'] = df['sepsis_occurred_True'].astype(float)
    for cov in covariates:
        df[cov] = df[cov].astype(float)
        if df[cov].isna().any():
            raise ValueError(f"Missing values in {cov}")

    # Initialize arrays for DR components
    psi1 = np.zeros(len(df))
    psi0 = np.zeros(len(df))

    # K-fold cross-fitting
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    for fold, (train_idx, test_idx) in enumerate(kf.split(df)):
        train_df = df.iloc[train_idx]
        test_df = df.iloc[test_idx]

        # Prepare covariate data
        X_train = train_df[covariates].values
        X_test = test_df[covariates].values
        y_train = train_df['sepsis_occurred_True']
        t_train = train_df['transfer_occurred_True']

        # Fit propensity score model (logistic regression)
        ps_model = LogisticRegression(random_state=42, max_iter=1000)
        ps_model.fit(X_train, t_train)
        ps_test = ps_model.predict_proba(X_test)[:, 1]

        # Clip propensity scores
        ps_test = np.clip(ps_test, 0.05, 0.95)

        # Fit outcome models (GLM with binomial family)
        treat_train = train_df[train_df['transfer_occurred_True'] == 1]
        control_train = train_df[train_df['transfer_occurred_True'] == 0]

        # Add constant for GLM
        X_train_treat = sm.add_constant(treat_train[covariates])
        X_train_control = sm.add_constant(control_train[covariates])
        X_test_const = sm.add_constant(X_test, has_constant='add')

        try:
            treat_model = sm.GLM(treat_train['sepsis_occurred_True'], X_train_treat, family=sm.families.Binomial()).fit()
            control_model = sm.GLM(control_train['sepsis_occurred_True'], X_train_control, family=sm.families.Binomial()).fit()
        except Exception as e:
            print(f"GLM fitting failed in fold {fold}: {e}")
            return np.nan, psi1, psi0, np.nan

        # Predict outcomes
        mu1hat_test = treat_model.predict(X_test_const)
        mu0hat_test = control_model.predict(X_test_const)

        # Compute DR components
        psi1[test_idx] = mu1hat_test + test_df['transfer_occurred_True'] * (test_df['sepsis_occurred_True'] - mu1hat_test) / ps_test
        psi0[test_idx] = mu0hat_test + (1 - test_df['transfer_occurred_True']) * (test_df['sepsis_occurred_True'] - mu0hat_test) / (1 - ps_test)

    # Check for NaNs
    if np.isnan(psi1).any() or np.isnan(psi0).any():
        print("Warning: NaNs detected in DR components")
        return np.nan, psi1, psi0, np.nan

    # Compute ATE and variance
    dr_est = np.mean(psi1 - psi0)
    dr_var = np.var(psi1 - psi0, ddof=1) / len(df)  # Sample variance / n
    return dr_est, psi1, psi0, dr_var

def main():
    # Define covariates (update with additional confounders if available)
    covariates = ['gcs_before_transfer', 'comorbidity_count']
    # If available: covariates = ['gcs_before_transfer', 'comorbidity_count', 'age', 'sex', 'admission_type', 'initial_sofa']

    # Compute DR ATE and variance
    dr_est, psi1, psi0, dr_var = compute_dr_ate(dataset, covariates=covariates)

    # Compute analytic CI
    if not np.isnan(dr_var):
        z = norm.ppf(0.975)  # Equivalent to qnorm(0.975)
        ci_lower_analytic = dr_est - z * np.sqrt(dr_var)
        ci_upper_analytic = dr_est + z * np.sqrt(dr_var)
    else:
        ci_lower_analytic, ci_upper_analytic = np.nan, np.nan

    # Print results
    print(f"Point Estimate DR ATE: {dr_est:.4f}")
    print(f"Analytic Variance: {dr_var:.6f}")
    print(f"Analytic 95% Confidence Interval: [{ci_lower_analytic:.4f}, {ci_upper_analytic:.4f}]")

    # Save results
    results = pd.DataFrame({
        'DR_ATE': [dr_est],
        'Analytic_Variance': [dr_var],
        'Analytic_CI_Lower': [ci_lower_analytic],
        'Analytic_CI_Upper': [ci_upper_analytic],

    })
    results.to_csv('dr_ate_results.csv', index=False)

if __name__ == "__main__":
    main()

